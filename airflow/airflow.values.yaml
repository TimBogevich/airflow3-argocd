airflow:
  fernetKey: "pT8ZDjwCvnWkfPEYBm12q2p9srNkM-nWC6Ss9aAcMEw="
  # defaultAirflowRepository: "hollister/airflow3"
  airflowVersion: "2.10.4"
  defaultAirflowTag: "2.10.5-python3.11"

  useStandardNaming: true

  ingress:
    apiServer:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: nginx
      host: airflow.local
      ingressClassName: nginx
      tls:
        enabled: false

  createUserJob:
    applyCustomEnv: false
    useHelmHooks: false

  migrateDatabaseJob:
    applyCustomEnv: false
    jobAnnotations:
      "argocd.argoproj.io/hook": Sync
    useHelmHooks: false

  dags:
    gitSync:
      enabled: true
      repo: https://gitlab.goodsforecast.ru/integration/etl2/etl-product.git
      branch: airflow3_migration
      rev: HEAD
      depth: 3
      subPath: "dags"
      credentialsSecret: git-credentials

  workers:
    extraContainers:
      - name: dag-trigger
        image: apache/airflow:2.10.5-python3.11
        env:
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                name: airflow-metadata
                key: connection
          - name: AIRFLOW__CELERY__BROKER_URL
            valueFrom:
              secretKeyRef:
                name: airflow-broker-url
                key: connection
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                name: airflow-fernet-key
                key: fernet-key
        command:
          - /bin/bash
          - -c
        args:
          - |
            echo ">>> Waiting for Airflow database initialization..."
            until airflow db check > /dev/null 2>&1; do
              echo "Database not ready, waiting..."
              sleep 10
            done
            echo ">>> Database is ready!"
            
            echo ">>> Waiting for webserver readiness..."
            until curl -sf http://airflow-webserver:8080/health > /dev/null; do
              sleep 5
            done
            echo ">>> Webserver is ready!"
            
            echo ">>> Triggering DAG..."
            airflow dags unpause load_excel || true
            airflow dags trigger load_excel || true
            echo ">>> DAG triggered successfully"
            tail -f /dev/null
        volumeMounts:
          - name: airflow-config-volume
            mountPath: /opt/airflow/airflow.cfg
            subPath: airflow.cfg

    extraVolumes:
      - name: airflow-config-volume
        configMap:
          name: airflow-config

  extraConfigMaps:
    "{{ .Release.Name }}-airflow-variables":
      labels:
        my.custom.label/v2: my_custom_label_value_2
      data: |
        PYTHONPATH: "/opt/airflow/dags/repo/libs:/opt/airflow/dags/repo/libs_custom"

  extraEnvFrom: |
    - configMapRef:
        name: '{{ .Release.Name }}-airflow-variables'
